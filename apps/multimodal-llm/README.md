# Multimodal LLM

Enhances language understanding and generation by combining text and image data. It uses [Amazon Bedrock's Claude 3](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html). 

Introduce how to invoke multimodal LLM using Bedrock API and LangChain.

## Requirements

- First, you need to access Bedrock's models by referring to this [document](https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/model-access.html).
- Check this [link](./https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/models-supported.html) to see if the model supports both Text and Image input modalities. (In this example, use Claude 3.5 Sonnet)
